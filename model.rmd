---
title: "Model Building"
author: "Venkata Battepati"
date: "02/03/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(psych) # Contains the winsor function and other helpful statistical tools
library(tidyverse) # Remember from last homework that dplyr is included in tidyverse
library(gridExtra)
library(corrplot)
library(patchwork)
library(grid)
library(factoextra)
library(FactoMineR)
library(mice) # Multiple imputation - may not use
library(xgboost)
library(klaR) # kmodes function 
library(PCAmixdata)
library(MASS)
library(caTools)
library(class)
set.seed(580)
```


Reading datasets

```{r}
control <- read.csv("Sepsis_control_data.csv")
positive <- read.csv("Sepsis_positive_data.csv")
positive$sepsis_group <- NULL
control$sepsis_group <- NULL
head(control)
head(positive)
```

Summary of datasets

```{r}
names <- c('mech_vent' ,'apnea','perfusion','co_ivh_shunt','co_surgical','co_congenital','co_chronic_lung','co_nec','ecmo','uac','cvl','lethargy','temp_threshold','fio2_threshold')
control[,names] <- lapply(control[,names] , factor)
summary(control)
```

```{r}
names <- c('mech_vent' ,'apnea','perfusion','co_ivh_shunt','co_surgical','co_congenital','co_chronic_lung','co_nec','ecmo','uac','cvl','lethargy','temp_threshold','fio2_threshold')
positive[,names] <- lapply(positive[,names] , factor)
summary(positive)
```


Combining data sets

```{r}
control$sepsis = 0
positive$sepsis = 1
df = rbind(control,positive)
head(df)
nrow(df)
```

```{r}
names <- c('mech_vent' ,'apnea','perfusion','co_ivh_shunt','co_surgical','co_congenital','co_chronic_lung','co_nec','ecmo','uac','cvl','lethargy','temp_threshold','fio2_threshold','sepsis')
df[,names] <- lapply(df[,names] , factor)
summary(df)
```

```{r}
numeric_df =  select_if(df, is.numeric)
corr_df = cor(numeric_df,use = "complete.obs")
corrplot(corr_df, method = 'circle', title="Correlation plot for all Numeric Variables", mar=c(1,0,1,0))
```

Creating a matrix from most correlated variables

```{r}
plot_data = df[, c('dbp',  'map',  'glucose', 'sbp', 'temp', 'rr')]
```


```{r}
panel.hist <- function(x, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5))
    his <- hist(x, plot = FALSE)
    breaks <- his$breaks
    nB <- length(breaks)
    y <- his$counts
    y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = rgb(0, 1, 1, alpha = 0.5), ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    Cor <- cor(x,y,use = "complete.obs")
    txt <- paste0(prefix, format(c(Cor, 0.123456789), digits = digits)[1])
    if(missing(cex.cor)) {
        cex.cor <- 0.4 / strwidth(txt)
    }
    text(0.5, 0.5, txt,
         cex = 1 + cex.cor * Cor) # Resize the text by level of correlation
}
pairs(plot_data, upper.panel = panel.cor,diag.panel = panel.hist,lower.panel = panel.smooth,main = "Scatter plots, histograms, and corr coeff for variables of interest")
```

Creating boxplot for variables

```{r}
box1 = ggplot(df, aes(x = sepsis, y = glucose))+ #choses what dats is used for boxplot
  geom_boxplot()+ #makes boxplot 
  labs(title="Glucose")+
  theme(plot.title = element_text(hjust = 0.5))
box2 = ggplot(df, aes(x = sepsis,y = map))+
  geom_boxplot()+
  labs(title="Mean Arterial Press.")+
  theme(plot.title = element_text(size = 10,hjust = 0.5))
box3 = ggplot(df, aes(x = sepsis,y = temp))+
  geom_boxplot()+
  labs(title="Temperature")+
  theme(plot.title = element_text(hjust = 0.5))
box4 = ggplot(df, aes(x = sepsis,y = rr))+
  geom_boxplot()+
  labs(title="Respiratory Rate")+
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(box1,box2,box3,box4,
          ncol = 4, top = textGrob("Boxplots for variables of interest")) #creates 1x3 plots
```



Next step: Should we impute or remove all missing values? If impute - which imputation?

Assessing missingness

```{r}
library(visdat)
num = c(1:17, 19, 27:30)
vis_miss(df[num])
```

Removing capPH due to high missingness - 93.56% - and imputing the rest

```{r}
df = df %>% dplyr::select(-capPH)
```

Assessing co-missingness of test panels
```{r}
num_binary = df # Columns 3-9 have most missingness

# Converting NA's to 1 (i.e. missing) and numerical values to 0 (i.e. not missing)
for (i in 3:9) {
  num_binary[,i] = ifelse(is.na(num_binary[,i]) == T,1,0)
}


corr_df_miss = cor(num_binary[3:9],use = "complete.obs")
corrplot(corr_df_miss, method = 'number', title="Missingness Correlation Plot", mar=c(1,0,1,0))
```

Two obvious correlation groups, or potential "test panels" of tests administered together, can be seen: 1) Complete blood cell panel (WBC, HGB, it_ratio, platelet_count) and 2) Blood chemistry panel (creatinine, bicarb, glucose). A missingness category will be created for each group to reduce the number of features.


Creating missingness category df with mean imputation of features with small missingness (<50%)
```{r}
df_missingCats = num_binary %>% dplyr::select(-c('wbc','hgb', 'platelet_count', 'bicarb', 'glucose'))
colnames(df_missingCats)[3] = "cbc_missingness"
colnames(df_missingCats)[4] = "blood_chem_missingness"
for (i in 1:ncol(df_missingCats)) {
  df_missingCats[,i][is.na(df_missingCats[,i])] = mean(df_missingCats[,i], na.rm=TRUE)
}
```

Mean imputation df
```{r}
# Perform mean imputation on numeric variables
df_meanImp = df
for (i in 1:ncol(df_meanImp)) {
  df_meanImp[,i][is.na(df_meanImp[,i])] = mean(df_meanImp[,i], na.rm=TRUE)
}
vis_miss(df_meanImp[num])
```



Unsupervised Learning - PCA for numeric variables

Center/scale data and prepare PCA

```{r}
# First must convert factors back into numeric values
df_meanImp[,names] = lapply(df_meanImp[,names] , as.character) # Convert to char first to avoid 0-1 ordinal values becoming 1-2
df_missingCats[,names] = lapply(df_missingCats[,names], as.character)
df_meanImp[,names] = lapply(df_meanImp[,names] , as.numeric)
df_missingCats[,names] = lapply(df_missingCats[,names], as.numeric)

# SCALING ACTUAL DATASETS FOR USE IN MODELS
df_meanImp[c(1:16, 18, 26:29)] = scale(df_meanImp[c(1:16, 18, 26:29)], center=T, scale=T)
df_missingCats[c(1:2, 5:11, 13, 21:24)] = scale(df_missingCats[c(1:2, 5:11, 13, 21:24)], center=T, scale=T)

# Scaled numeric datasets
df_mean_numeric = df_meanImp[c(1:16, 18, 26:29)]
df_missing_numeric = df_missingCats[c(1:2, 5:11, 13, 21:24)]

mean_pr.out = prcomp(df_mean_numeric)
missing_pr.out = prcomp(df_missing_numeric)
summary(mean_pr.out)
summary(missing_pr.out)
```

Scree plot to visualize % variance explained

```{r}
fviz_eig(mean_pr.out, addlabels = TRUE, main='Mean Imputed PCA')
fviz_eig(missing_pr.out, addlabels=T, main="Missing Categories PCA")
```
For the mean imputed df, the elbow is visible around PC4 which only accounts for 44.5% variance. For the missingness categories df, the elbow is visible around PC3 or PC5 - 53.4% or 72.4% variance.

Displaying eigenvalue scree plot to analyze which PCs have eigenvalue greater than 1.

```{r}
fviz_eig(mean_pr.out, choice = 'eigenvalue', main="Mean PCA Eigenvalues")+
  geom_hline(yintercept=1, color = 'red')

fviz_eig(missing_pr.out, choice = 'eigenvalue', main='Missingness PCA Eigenvalues')+
  geom_hline(yintercept=1, color='red')

# Displaying eigenvalues since scree plot doesn't display all PCs
get_eig(mean_pr.out)
get_eig(missing_pr.out)
```
For the mean imputed PCA, the first 9 PCs have eigenvalues greater than one. For the missing category PCA, the first 5 PCs have an eigenvalue greater than one.

Biplot of PC1 and PC2 to assess what variables contribute the most

```{r}
fviz_pca_biplot(mean_pr.out, label="var", col.ind="cos2", title="Sepsis Mean Imputed PCA Biplot", repel=T, invisible='ind')+
  theme_minimal()

fviz_pca_biplot(missing_pr.out, label="var", col.ind="cos2", title="Sepsis Missingness Category PCA Biplot", repel=T, invisible='ind')+
  theme_minimal()
```


 The mean imputed PCA biplot shows the strongest signficance towards PC1 and PC2 comes from gestational age, age, weight, and sbp/dbp/map. The missingness category PCA biplot shows the strongest contributions from these same values as well as slightly larger contributions from hr and temp.


Unsupervised Learning - MCA for ordinal variables

```{r}
# Create dataset of binary classification variables, convert to char
mean_mca_df = df_meanImp[c(17, 19:25, 30:35)]
ord_list = c('mech_vent' ,'apnea','perfusion','co_ivh_shunt','co_surgical','co_congenital','co_chronic_lung','co_nec','ecmo','uac','cvl','lethargy','temp_threshold','fio2_threshold')
mean_mca_df[, ord_list] = lapply(mean_mca_df[, ord_list], as.character)

missing_mca_df = df_missingCats[c(3:4, 12, 14:20, 25:30)]
miss_ord_list = c('cbc_missingness', 'blood_chem_missingness', 'mech_vent' ,'apnea','perfusion','co_ivh_shunt','co_surgical','co_congenital','co_chronic_lung','co_nec','ecmo','uac','cvl','lethargy','temp_threshold','fio2_threshold')
missing_mca_df[, miss_ord_list] = lapply(missing_mca_df[, miss_ord_list], as.character)

#Perform MCA and plots
mean_mca.out = MCA(mean_mca_df, ncp = 10, graph = FALSE)
fviz_eig(mean_mca.out, addlabels = TRUE)

fviz_mca_biplot(mean_mca.out, repel = TRUE, label="var", col.ind="cos2", title="Sepsis Mean Imp MCA Biplot")+
  theme_minimal()


missing_mca.out = MCA(missing_mca_df, ncp = 10, graph = FALSE)
fviz_eig(missing_mca.out, addlabels = TRUE)

fviz_mca_biplot(missing_mca.out, repel = TRUE, label="var", col.ind="cos2", title="Sepsis Missing MCA Biplot")+
  theme_minimal()
```
Dimensions 1 and 2 do not adequately encompass most of the variance within the ordinal data for either dataset, but it is evident that ecmo_1 and uac_1 contribute the most significantly to these two components. Further analysis should be considered on these variables.


PCAmixdata Attempt

```{r}
# Mean imputed data
library(PCAmixdata)
pcamix.out <- PCAmix(X.quanti = df_mean_numeric, X.quali = mean_mca_df, ndim=5, rename.level=T, graph=F)
summary(pcamix.out)
pcamix.out$eig


```

```{r}
# Missingness categories data
library(PCAmixdata)
pcamix_missout = PCAmix(X.quanti = df_missing_numeric, X.quali = missing_mca_df, ndim=5, rename.level=T, graph=F)
summary(pcamix_missout)
pcamix_missout$eig

```


Unsupervised Learning - K-means/K-modes clustering

K-means for numerical data

Silhouette method to assess optimal K value (chosen over elbow method due to ambiguity with elbow method)

```{r}
#Silhouette method
fviz_nbclust(df_mean_numeric, kmeans, method="silhouette")+
  labs(subtitle="Mean Imputation Silhouette")

fviz_nbclust(df_missing_numeric, kmeans, method="silhouette")+
  labs(subtitle="Missing Silhouette")
```

```{r}
# Mean imputation df cluster
kmeans_meanImp = kmeans(df_mean_numeric, centers=2, iter.max=10, nstart=20)
fviz_cluster(kmeans_meanImp, df_mean_numeric)+
  labs(title="K=2 Mean Imputation Cluster Plot")

# Missingness df cluster
kmeans_missingCats = kmeans(df_missing_numeric, centers=3, iter.max=10, nstart=20)
fviz_cluster(kmeans_missingCats, df_missing_numeric)+
  labs(title="K=3 Missing Categories Cluster Plot")

# Tables
table(Cluster = kmeans_meanImp$cluster, Outcome = df_meanImp$sepsis)
table(Cluster = kmeans_missingCats$cluster, Outcome = df_missingCats$sepsis)
```


K-modes for categorical data

```{r}
# Using same number of modes as number of K centers above
# No good cluster visualization for kmodes, so only printing tables
kmodes_meanImp = kmodes(mean_mca_df, modes=2, iter.max=10, fast=F)
table(Cluster = kmodes_meanImp$cluster, Outcome = df_meanImp$sepsis)

kmodes_missingCats = kmodes(missing_mca_df, modes=3, iter.max=10, fast=F)
table(Cluster = kmodes_missingCats$cluster, Outcome = df_missingCats$sepsis)

```

```{r}
write.csv(df_mean_numeric,"C:\\Users\\venka\\OneDrive\\Documents\\GitHub\\BME-580-Project\\df_mean_numeric.csv", row.names = FALSE)

write.csv(df_missing_numeric,"C:\\Users\\venka\\OneDrive\\Documents\\GitHub\\BME-580-Project\\df_missing_numeric.csv", row.names = FALSE)

write.csv(df_meanImp,"C:\\Users\\venka\\OneDrive\\Documents\\GitHub\\BME-580-Project\\df_meanImp.csv", row.names = FALSE)

write.csv(df_missingCats,"C:\\Users\\venka\\OneDrive\\Documents\\GitHub\\BME-580-Project\\df_missingCats.csv", row.names = FALSE)
```


Data division to test and train

```{r}
split = sample.split(df_meanImp$sepsis, SplitRatio=.8)

train <- subset(df_meanImp, split==TRUE) 
test <- subset(df_meanImp,split ==FALSE)
```

spliti = sample.split(positive$sepsis, SplitRatio=.8)

traini <- subset(positive, split==TRUE) 
testi <- subset(positive, split ==FALSE)



Model building

```{r}
pc.comp <- pcamix.out$scores
pc.comp1 <- -1*pc.comp[,1] # principal component 1 scores (negated for convenience)
pc.comp2 <- -1*pc.comp[,2] # principal component 2 scores (negated for convenience)
pc.comp3 <- -1*pc.comp[,3]
pc.comp4 <- -1*pc.comp[,4]
pc.comp5 <- -1*pc.comp[,5]

logModel <- glm(sepsis~pc.comp1+pc.comp2+pc.comp3+pc.comp4+pc.comp5, family=binomial, data = train)
summary(logModel)
```


``{r}
logModel = glm(sepsis ~., family = binomial, data = train)
summary(logModel)
``


``{r}
log.fit = predict(logModel, newdata = test, type = 'response')
s

misclassification = sum(log.fit != testData$type)/length(testData$type)
print(paste('Accuracy', 1-misclassification))

